# Cryptostore sample config file for Docker

cache: redis

redis:
    ip: redisdb
    port: 6379
    del_after_read: true
    start_flush: true
    socket: null

exchanges:
    BINANCE:
        trades: [BTC-USDT, ETH-USDT]
        ticker: [BTC-USDT, ETH-USDT]

# Where to store the data. Currently arctic, influx, elastic, and parquet are supported. More than one can be enabled
storage: [parquet]
# These two values configure how many times to retry a write (before dying)
# and how long to wait between attempts to write after a failure is encountered.
storage_retries: 5
storage_retry_wait: 30

# Parquet specific options. Parquet will default to storing the data on disk unless these are specified
parquet:
    # If storing the data to an external source (like S3) toggle this to enable the removal of the local file after
    # writing to external store
    del_file: false
    # Append X times a parquet file before creating a new one to store data.
    # This feature allows dumping frequently data from Redis (as defined by 'storage_interval'), while writing less frequently new parquet files.
    # Remove this parameter or set it to 0 to write a new file each 'storage_interval' (appending is then disabled).
    # Beware that a parquet file not properly closed cannot be read back. Closing is made at the last appending iteration.
    append_counter: 4
    # File name format, as a list of arguments. Default is <exchange>-<data_type>-<symbol>-<timestamp>.parquet, which would be exchange, data_type, symbol, timestamp
    # Can specify any combination exchange, data_type, symbol, and timestamp
    # timestamp MUST be present.
    # examples:
    #    symbol,timestamp
    #    timestamp
    #    timestamp,exchange
    #    etc
    file_format: [exchange, symbol, data_type, timestamp]
    # A compression codec (and level if relevant) can be defined to compress data when writing parquet file.
    # Compression is managed by use of `pyarrow.parquet.write_table()` interface.
    # Compression codecs (and levels in parentheses) are: NONE, SNAPPY, GZIP (1 to 9), BROTLI (1 to 11), LZ4 (1 is forced) & ZSTD (1 to 22).
    # You can remove these keywords if you don't want to compress the data. As a proposal, BROTLI appears especially efficient.
    compression:
        codec: 'BROTLI'
        level: 6
    # Path controls where the file is written (if not using S3/GC) If null, will write to CWD. Must be absolute path.
    path: null
    # Automatically groups all paquet files in a folder YYYY-MM-DD both on the local filesystem, as well as on the cloud storage engine.
    # Setting prefix on the cloud storage engine will override this setting.
    prefix_date: false

# Data batching window, in seconds, or optionally a time interval: M(inutely), H(ourly), D(aily).
# These intervals will require that you have enough memory available to redis (and python) to hold this amount of data.
# String intervals can be combined with numbers, eg 2H is 2 hours, 5M is 5 minutes, etc.
# Note that if a time interval is selected and kafka is used, the timestamps used to aggregate the data
# for the time intervals from from Kafka's internal timestamp offsets - these will differ from the exchange
# provided timestamps - it may be anywhere from 0.001 ms to seconds depending on your kafka cluster and
# other hardware and setup configurations.
storage_interval: 5
